# 第一阶段测试指南

## 测试目标

验证事件中心的事件发布和消费功能是否正常工作，包括：
1. API 服务发布 API 生命周期事件
2. 治理服务发布告警事件
3. 事件服务消费事件并分发给订阅者

## 前置准备

### 1. 启动基础服务

确保以下服务已启动：
- MySQL（数据库）
- Redis（缓存）
- Kafka（消息队列）
- Nacos（服务注册中心）

### 2. 初始化事件定义

执行事件定义初始化脚本：

```sql
-- 文件位置：intelli-event-service/src/main/resources/db/init_events.sql
-- 在事件服务的数据库中执行
```

### 3. 启动服务

按顺序启动以下服务：
1. **事件服务**（intelli-event-service）- 端口 8087
2. **API 服务**（intelli-api-platform-service）
3. **治理服务**（intelli-governance-service）

## 测试场景一：API 发布事件

### 测试步骤

#### 1. 发布一个 API

**请求：**
```http
POST http://localhost:{api-service-port}/v1/apis/{apiId}/publish
Content-Type: application/json
Authorization: Bearer {token}
```

#### 2. 查看日志

**API 服务日志：**
```
发布API发布事件 - apiId: xxx, path: /v1/users
```

**事件服务日志：**
```
处理事件消息: eventCode=api.published, tenantId=xxx
查询事件订阅列表: eventCode=api.published, count=x
```

#### 3. 查看 Kafka 消息

使用 Kafka 工具查看 `intellihub-event` Topic 中的消息：

```bash
# 使用 kafka-console-consumer 查看消息
kafka-console-consumer.sh --bootstrap-server 192.168.200.130:9092 \
  --topic intellihub-event \
  --from-beginning
```

**预期消息格式：**
```json
{
  "tenantId": "tenant001",
  "eventCode": "api.published",
  "source": "api-platform-service",
  "data": {
    "apiId": "xxx",
    "apiName": "用户查询API",
    "apiPath": "/v1/users",
    "method": "GET",
    "publishedAt": "2024-01-15T10:30:00"
  },
  "timestamp": "2024-01-15T10:30:00"
}
```

#### 4. 查看数据库记录

**事件发布记录表：**
```sql
SELECT * FROM event_publish_record 
WHERE event_code = 'api.published' 
ORDER BY created_at DESC 
LIMIT 10;
```

**事件消费记录表：**
```sql
SELECT * FROM event_consume_record 
WHERE event_code = 'api.published' 
ORDER BY created_at DESC 
LIMIT 10;
```

### 预期结果

✅ API 服务成功发布事件到 Kafka  
✅ 事件服务成功消费事件  
✅ 事件发布记录已保存到数据库  
✅ 事件消费记录已保存到数据库（如果有订阅者）

## 测试场景二：API 下线事件

### 测试步骤

#### 1. 下线一个 API

**请求：**
```http
POST http://localhost:{api-service-port}/v1/apis/{apiId}/offline
Content-Type: application/json
Authorization: Bearer {token}
```

#### 2. 查看日志和数据

参考测试场景一的步骤 2-4。

**预期事件数据：**
```json
{
  "tenantId": "tenant001",
  "eventCode": "api.offline",
  "source": "api-platform-service",
  "data": {
    "apiId": "xxx",
    "apiPath": "/v1/users",
    "method": "GET",
    "offlineAt": "2024-01-15T10:35:00"
  },
  "timestamp": "2024-01-15T10:35:00"
}
```

## 测试场景三：API 删除事件

### 测试步骤

#### 1. 删除一个 API（需要先下线）

**请求：**
```http
DELETE http://localhost:{api-service-port}/v1/apis/{apiId}
Authorization: Bearer {token}
```

#### 2. 查看日志和数据

参考测试场景一的步骤 2-4。

**预期事件数据：**
```json
{
  "tenantId": "tenant001",
  "eventCode": "api.deleted",
  "source": "api-platform-service",
  "data": {
    "apiId": "xxx",
    "apiPath": "/v1/users",
    "method": "GET",
    "deletedAt": "2024-01-15T10:40:00"
  },
  "timestamp": "2024-01-15T10:40:00"
}
```

## 测试场景四：告警触发事件

### 测试步骤

#### 1. 创建告警规则

**请求：**
```http
POST http://localhost:{governance-service-port}/v1/alert-rules
Content-Type: application/json
Authorization: Bearer {token}

{
  "tenantId": "tenant001",
  "name": "错误率告警",
  "ruleType": "error_rate",
  "threshold": 5.0,
  "operator": "gt",
  "duration": 60,
  "status": "active"
}
```

#### 2. 触发告警

模拟一些失败的 API 调用，使错误率超过阈值（5%）。

#### 3. 等待告警检测任务执行

告警检测任务每分钟执行一次（`@Scheduled(fixedRate = 60000)`）。

#### 4. 查看日志

**治理服务日志：**
```
[告警触发] 规则[错误率告警] 触发告警! level=warning, message=...
发布告警触发事件 - alertId: xxx, ruleName: 错误率告警, level: warning
```

**事件服务日志：**
```
处理事件消息: eventCode=alert.triggered, tenantId=xxx
查询事件订阅列表: eventCode=alert.triggered, count=x
```

#### 5. 查看 Kafka 消息

**预期消息格式：**
```json
{
  "tenantId": "tenant001",
  "eventCode": "alert.triggered",
  "source": "governance-service",
  "data": {
    "alertId": "xxx",
    "ruleId": "xxx",
    "ruleName": "错误率告警",
    "alertLevel": "warning",
    "metric": "error_rate",
    "currentValue": "8.5",
    "threshold": "5.0",
    "apiId": null,
    "apiPath": "global",
    "triggeredAt": "2024-01-15T10:45:00",
    "message": "[错误率告警] 错误率 当前值: 8.50%, 阈值: 5.00%"
  },
  "timestamp": "2024-01-15T10:45:00"
}
```

#### 6. 查看数据库记录

**告警记录表：**
```sql
SELECT * FROM alert_record 
WHERE rule_name = '错误率告警' 
ORDER BY fired_at DESC 
LIMIT 10;
```

**事件发布记录表：**
```sql
SELECT * FROM event_publish_record 
WHERE event_code = 'alert.triggered' 
ORDER BY created_at DESC 
LIMIT 10;
```

### 预期结果

✅ 告警规则成功触发  
✅ 治理服务成功发布告警事件到 Kafka  
✅ 事件服务成功消费事件  
✅ 告警记录已保存到数据库  
✅ 事件发布记录已保存到数据库

## 测试场景五：事件消费和分发

### 前置条件

需要先配置事件订阅关系。

#### 1. 创建订阅

**请求：**
```http
POST http://localhost:8087/v1/event-subscriptions/create
Content-Type: application/json

{
  "tenantId": "tenant001",
  "eventCode": "api.published",
  "subscriberName": "测试订阅者",
  "subscriberType": "WEBHOOK",
  "callbackUrl": "http://localhost:9999/webhook/api-published",
  "callbackMethod": "POST",
  "retryStrategy": "FIXED",
  "maxRetryTimes": 3,
  "priority": 5
}
```

#### 2. 启动 Webhook 测试服务器

创建一个简单的 HTTP 服务器来接收 Webhook 回调：

```python
# webhook_test_server.py
from flask import Flask, request
import json

app = Flask(__name__)

@app.route('/webhook/api-published', methods=['POST'])
def api_published():
    data = request.json
    print(f"收到API发布事件: {json.dumps(data, indent=2, ensure_ascii=False)}")
    return {'status': 'success'}

@app.route('/webhook/alert-triggered', methods=['POST'])
def alert_triggered():
    data = request.json
    print(f"收到告警触发事件: {json.dumps(data, indent=2, ensure_ascii=False)}")
    return {'status': 'success'}

if __name__ == '__main__':
    app.run(port=9999)
```

运行：
```bash
python webhook_test_server.py
```

#### 3. 触发事件

发布一个 API（参考测试场景一）。

#### 4. 查看 Webhook 服务器日志

**预期输出：**
```
收到API发布事件: {
  "tenantId": "tenant001",
  "eventCode": "api.published",
  "source": "api-platform-service",
  "data": {
    "apiId": "xxx",
    "apiName": "用户查询API",
    "apiPath": "/v1/users",
    "method": "GET",
    "publishedAt": "2024-01-15T10:30:00"
  },
  "timestamp": "2024-01-15T10:30:00"
}
```

#### 5. 查看事件消费记录

```sql
SELECT * FROM event_consume_record 
WHERE subscription_id = '{订阅ID}' 
ORDER BY consume_time DESC 
LIMIT 10;
```

**预期字段：**
- `status`: `SUCCESS`
- `response_code`: `200`
- `retry_times`: `0`
- `duration_ms`: 消费耗时（毫秒）

### 预期结果

✅ 事件服务成功查询订阅列表  
✅ 事件服务成功调用 Webhook  
✅ Webhook 服务器成功接收事件  
✅ 消费记录已保存到数据库

## 常见问题排查

### 问题1：事件服务无法启动

**可能原因：**
- Kafka 依赖未正确编译

**解决方案：**
```bash
cd d:\Develop\Code_github\intelli_hub\intellihub-parent
# 在 IDE 中执行 Maven -> Reimport
# 或者使用 Maven 命令（如果配置了 Maven）
```

### 问题2：事件未发布到 Kafka

**排查步骤：**
1. 检查 Kafka 是否正常运行
2. 检查 API/治理服务日志是否有错误
3. 检查 Kafka 配置是否正确（bootstrap-servers）

**查看 Kafka Topic：**
```bash
kafka-topics.sh --bootstrap-server 192.168.200.130:9092 --list
```

### 问题3：事件服务未消费事件

**排查步骤：**
1. 检查事件服务是否正常启动
2. 检查 EventConsumer 是否有错误日志
3. 检查 Kafka Consumer Group 是否正常

**查看 Consumer Group：**
```bash
kafka-consumer-groups.sh --bootstrap-server 192.168.200.130:9092 \
  --describe --group event-service-group
```

### 问题4：Webhook 调用失败

**排查步骤：**
1. 检查 Webhook URL 是否可访问
2. 检查订阅配置是否正确
3. 查看事件消费记录表的 `error_message` 字段

**查询失败记录：**
```sql
SELECT * FROM event_consume_record 
WHERE status = 'FAILED' 
ORDER BY consume_time DESC 
LIMIT 10;
```

## 性能测试

### 测试目标

验证事件中心的性能和稳定性。

### 测试方案

#### 1. 并发发布测试

使用 JMeter 或其他压测工具，并发发布 API 事件：
- 并发数：100
- 持续时间：5 分钟
- 预期 TPS：1000+

#### 2. 消费延迟测试

监控事件从发布到消费的延迟：
- 预期延迟：< 1 秒（P95）
- 预期延迟：< 2 秒（P99）

#### 3. 消息堆积测试

停止事件服务，持续发布事件，观察 Kafka 消息堆积情况：
- 预期：消息正常堆积，不丢失
- 重启事件服务后，消息正常消费

### 监控指标

**Kafka 指标：**
- Topic 消息数量
- Consumer Lag（消费延迟）
- 分区分布

**数据库指标：**
- 事件发布记录数
- 事件消费记录数
- 失败记录数

**应用指标：**
- 事件发布成功率
- 事件消费成功率
- 平均消费耗时

## 测试检查清单

- [ ] Kafka 服务正常运行
- [ ] 事件定义已初始化
- [ ] 事件服务正常启动
- [ ] API 服务正常启动
- [ ] 治理服务正常启动
- [ ] API 发布事件测试通过
- [ ] API 下线事件测试通过
- [ ] API 删除事件测试通过
- [ ] 告警触发事件测试通过
- [ ] 事件消费和分发测试通过
- [ ] 所有事件都有发布记录
- [ ] 所有事件都有消费记录
- [ ] Webhook 回调成功
- [ ] 失败重试机制正常工作

## 下一步

测试通过后，可以进行：
1. 在网关服务中实现 API 事件订阅
2. 实现更多的订阅者（审计服务、文档服务、监控服务）
3. 开发事件中心前端管理界面
4. 实施第二阶段事件（用户事件、应用事件）
